---
blocks:
  - body: "# ***IndicXlit***\n\n[Website](https://ai4bharat.org/indic-xlit \"\")\_|\_[Downloads](https://github.com/AI4Bharat/IndicXlit#download-indicxlit-model)\_|\_[Paper](https://arxiv.org/abs/2205.03018)\_|\_[Demo](https://xlit.ai4bharat.org/)\_|\_[Python Library](https://pypi.org/project/ai4bharat-transliteration)\n\n[***IndicXlit***](https://indicnlp.ai4bharat.org/indic-xlit)\_is a transformer-based multilingual transliteration model (~11M) for roman to native script conversion that\_***supports 21 Indic languages***. It is trained on\_[***Aksharantar***](https://indicnlp.ai4bharat.org/aksharantar/)\_dataset which is the\_***largest publicly available parallel corpus containing 26 million word pairs spanning 20 Indic languages***\_at the time of writing (5 May 2022). It supports following 21 Indic languages:\n"
    _template: content
  - markdownTable: "\n| <!-- -->  \t | <!-- --> \t  | <!-- --> \t   | <!-- -->\t     | <!-- -->       | <!-- -->      |\n| -------------- | -------------- | -------------- | --------------- | -------------- | ------------- |\n| Assamese (asm) | Bengali (ben)  |  Bodo (brx)    | Gujarati (guj)  | Hindi (hin)    | Kannada (kan) |\n| Kashmiri (kas) | Konkani (gom)  | Maithili (mai) | Malayalam (mal) | Manipuri (mni) | Marathi (mar) | \n| Nepali (nep)   | Oriya (ori)    | Punjabi (pan)  |  Sanskrit (san) | Sindhi (snd)   | Sinhala (sin) |\n|  Tamil (tam)   |  Telugu (tel)  |   Urdu (urd)   | "
    color: default
    _template: table
  - body: "### Evaluation Results\n\nIndicXlit is evaluated on\_[Dakshina benchmark](https://github.com/google-research-datasets/dakshina)\_and\_[Aksharantar benchmark](https://ai4bharat.org/aksharantar/ \"\"). IndicXlit achieves state-of-theart results on the Dakshina testset and also provide baseline results on the new Aksharantar testset. The Top-1 results are summarized below. For more details, refer\_our [paper](https://arxiv.org/abs/2205.03018)\n"
    _template: content
  - markdownTable: >
      | Languages | asm | ben | brx | guj | hin | kan | kas | kok | mai | mal |
      mni | mar | nep | ori | pan | san | tam | tel | urd |

      | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
      --- | --- | --- | --- | --- | --- | --- | --- |

      | Dakshina | - | 55.49 | - | 62.02 | 60.56 | 77.18 | - | - | - | 63.56 | -
      | 64.85 | - | - | 47.24 | - | 68.10 | 73.38 | 42.12 | 61.45 |

      | Aksharantar (native words) | 60.27 | 61.70 | 70.79 | 61.89 | 55.59 |
      76.18 | 28.76 | 63.06 | 72.06 | 64.73 | 83.19 | 63.72 | 80.25 | 58.90 |
      40.27 | 78.63 | 69.78 | 84.69 | 48.37 |

      | Aksharantar (named entities) | 38.62 | 37.12 | 30.32 | 48.89 | 58.87 |
      49.92 | 20.23 | 34.36 | 42.82 | 33.93 | 44.12 | 53.57 | 52.67 | 30.63 |
      36.08 | 24.06 | 42.12 | 51.82 | 47.77 |
    color: default
    _template: table
  - body: "## Table of contents\n\n*   [Table of contents](https://github.com/AI4Bharat/IndicXlit#table-of-contents)\n*   [Resources](https://github.com/AI4Bharat/IndicXlit#resources)\n    *   [Download IndicXlit model](https://github.com/AI4Bharat/IndicXlit#download-indicxlit-model)\n    *   [Using hosted APIs](https://github.com/AI4Bharat/IndicXlit#using-hosted-apis)\n    *   [Accessing on ULCA](https://github.com/AI4Bharat/IndicXlit#accessing-on-ulca)\n*   [Running Inference](https://github.com/AI4Bharat/IndicXlit#running-inference)\n    *   [Command line interface](https://github.com/AI4Bharat/IndicXlit#command-line-interface)\n    *   [Python Inference](https://github.com/AI4Bharat/IndicXlit#python-inference)\n*   [Training model](https://github.com/AI4Bharat/IndicXlit#training-model)\n    *   [Setting up your environment](https://github.com/AI4Bharat/IndicXlit#setting-up-your-environment)\n    *   [Details of models and hyperparameters](https://github.com/AI4Bharat/IndicXlit#details-of-models-and-hyperparameters)\n    *   [Training procedure and code](https://github.com/AI4Bharat/IndicXlit#training-procedure-and-code)\n    *   [WandB plots](https://github.com/AI4Bharat/IndicXlit#wandb-plots)\n    *   [Evaluating trained model](https://github.com/AI4Bharat/IndicXlit#evaluating-trained-model)\n    *   [Detailed benchmarking results](https://github.com/AI4Bharat/IndicXlit#detailed-benchmarking-results)\n*   [Finetuning model on your data](https://github.com/AI4Bharat/IndicXlit#finetuning-model-on-your-data)\n*   [Mining details](https://github.com/AI4Bharat/IndicXlit#mining-details)\n*   [Directory structure](https://github.com/AI4Bharat/IndicXlit#directory-structure)\n*   [Citing](https://github.com/AI4Bharat/IndicXlit#citing)\n    *   [License](https://github.com/AI4Bharat/IndicXlit#license)\n    *   [Contributors](https://github.com/AI4Bharat/IndicXlit#contributors)\n    *   [Contact](https://github.com/AI4Bharat/IndicXlit#contact)\n\n## Resources\n\n### Download IndicXlit model\n\nRoman to Indic model\_[v1.0](https://storage.googleapis.com/indic-xlit-public/final\\_model/indicxlit-en-indic-v1.0.zip)\n\n### Using hosted APIs\n\nClick to expand\n\n### Accessing on ULCA\n\nYou can try out our model at\_[ULCA](https://bhashini.gov.in/ulca/model/explore-models)\_and filter for IndicXlit model.\n\n## Running Inference\n\n### Command line interface\n\nThe model is trained on words as inputs. hence, users need to split sentence into words before running the transliteratation model when using our command line interface.\n\nFollow the Colab notebook to setup the environment, download the trained\_*IndicXlit*\_model and transliterate your own text. GPU support is given in command line interface.\n\nCommand line interface -->\_\n\n### Python Inference\n\nPython interface -->\_\n\nThe python interface is useful in case you want to reuse the model for multiple translations and do not want to reinitialize the model each time. Moreover, re-ranking option is available in python interface, but not in command line interface.\n\n## Training model\n\n### Setting up your environment\n\nClick to expand\n\n## Details of models and hyperparameters\n\n*   Architecture: IndicXlit uses 6 encoder and decoder layers, input embeddings of size 256 with 4 attention heads and feedforward dimension of 1024 with total number of parameters of 11M\n*   Loss: Cross entropy loss\n*   Optimizer: Adam\n*   Adam-betas: (0.9, 0.98)\n*   Peak-learning-rate: 0.001\n*   Learning-rate-scheduler: inverse-sqrt\n*   Temperature-sampling (T): 1.5\n*   Warmup-steps: 4000\n\nPlease refer to section 6 of our\_[paper](https://arxiv.org/abs/2205.03018)\_for more details on training setup.\n\n### Training procedure and code\n\nThe high level steps we follow for training are as follows:\n\nOrganize the train/test/valid data in corpus dir such that it has all the files containing parallel data for en-X lang pair in the following format\n\ntrain\\_x.en for training file of en-X lang pair which contains the space separated roman characters in each line\n\ntrain\\_x.x for training file of en-X lang pair which contains the space separated Indic characters in each line\n\n```\n# corpus/\r\n# ├── train_as.as\r\n# ├── train_en.en\r\n# ├── train_bn.bn\r\n# ├── train_en.en\r\n# ├── ....\r\n# ├── valid_as.as\r\n# ├── valid_en.en\r\n# ├── valid_bn.bn\r\n# ├── valid_en.en\r\n# ├── ....\r\n# ├── test_as.as\r\n# ├── test_en.en\r\n# ├── test_bn.bn\r\n# ├── test_en.en\r\n# └── ....\r\n\n```\n\nJoint the training files across all languages\n\n```\n# corpus/\r\n# ├── train_combine.cmb\r\n# └── train_combine.en\n```\n\nCreate the joint vocabulary using all the combined training data.\n\n```\nfairseq-preprocess \\\r\n   --trainpref corpus/train_combine  \\\r\n   --source-lang en --target-lang cmb \\\r\n   --workers 256 \\\r\n   --destdir corpus-bin\n```\n\nCreate the binarized data required for fairseq for each langauge separately using joint vocabulary\n\n```\nfor lang_abr in bn gu hi kn ml mr pa sd si ta te ur\r\ndo\r\n   fairseq-preprocess \\\r\n   --trainpref corpus/train_$lang_abr --validpref corpus/valid_$lang_abr --testpref corpus/test_$lang_abr \\\r\n   --srcdict corpus-bin/dict.en.txt \\\r\n   --tgtdict corpus-bin/dict.cmb.txt \\\r\n   --source-lang en --target-lang $lang_abr \\\r\n   --workers 32 \\\r\n   --destdir corpus-bin \r\ndone\n```\n\nAdd all languages codes to\_`lang_list.txt`\_file and save it in the same dir\n\nStart training with fairseq-train command. Please refer to\_[fairseq documentaion](https://fairseq.readthedocs.io/en/latest/command\\_line\\_tools.html)\_to know more about each of these options\n\n```\n# training script\r\nfairseq-train corpus-bin \\\r\n  --save-dir transformer \\\r\n  --arch transformer --layernorm-embedding \\\r\n  --task translation_multi_simple_epoch \\\r\n  --sampling-method \"temperature\" \\\r\n  --sampling-temperature 1.5 \\\r\n  --encoder-langtok \"tgt\" \\\r\n  --lang-dict lang_list.txt \\\r\n  --lang-pairs en-bn,en-gu,en-hi,en-kn,en-ml,en-mr,en-pa,en-sd,en-si,en-ta,en-te,en-ur  \\\r\n  --decoder-normalize-before --encoder-normalize-before \\\r\n  --activation-fn gelu --adam-betas \"(0.9, 0.98)\"  \\\r\n  --batch-size 1024 \\\r\n  --decoder-attention-heads 4 --decoder-embed-dim 256 --decoder-ffn-embed-dim 1024 --decoder-layers 6 \\\r\n  --dropout 0.5 \\\r\n  --encoder-attention-heads 4 --encoder-embed-dim 256 --encoder-ffn-embed-dim 1024 --encoder-layers 6 \\\r\n  --lr 0.001 --lr-scheduler inverse_sqrt \\\r\n  --max-epoch 51 \\\r\n  --optimizer adam  \\\r\n  --num-workers 32 \\\r\n  --warmup-init-lr 0 --warmup-updates 4000\n```\n\nThe above steps are further documented in our colab notebook\_\n\nPlease refer to section 6 of our\_[paper](https://arxiv.org/abs/2205.03018)\_for more details of our training hyperparameters.\n\n### WandB plots\n\n[IndicXlit en-indic model](https://wandb.ai/cs20s002/transliteration\\_model/runs/3gdvqx6e?workspace=user-cs20s002)\n\n### Evaluating trained model\n\nThe trained model will get saved in the transformer directory. It will have the following files:\n\n```\n# transformer/\r\n# └── checkpoint_best.pt\n```\n\nTo generate the outputs after training, use following generation script which will generate the predictions and save it in output dir.\n\n```\nfor lang_abr in as bn brx gom gu hi kn ks mai ml mni mr ne or pa sa sd si ta te ur\r\ndo\r\nsource_lang=en\r\ntarget_lang=$lang_abr\r\nfairseq-generate corpus-bin \\\r\n  --path transformer/checkpoint_best.pt \\\r\n  --task translation_multi_simple_epoch \\\r\n  --gen-subset test \\\r\n  --beam 4 \\\r\n  --nbest 4 \\\r\n  --source-lang $source_lang \\\r\n  --target-lang $target_lang \\\r\n  --batch-size 4096 \\\r\n  --encoder-langtok \"tgt\" \\\r\n  --lang-dict lang_list.txt \\\r\n  --num-workers 64 \\\r\n  --lang-pairs en-as,en-bn,en-brx,en-gom,en-gu,en-hi,en-kn,en-ks,en-mai,en-ml,en-mni,en-mr,en-ne,en-or,en-pa,en-sa,en-sd,en-si,en-ta,en-te,en-ur  > output/${source_lang}_${target_lang}.txt\r\ndone\n```\n\nTo test the models after training, use\_`generate_result_files.py`\_to convert the fairseq output file into xml files and\_`evaluate_result_with_rescore_option.py`\_to compute accuracies.\n\n`evaluate_result_with_rescore_option.py`\_can be downloaded using following link,\n\n```\nwget https://storage.googleapis.com/indic-xlit-public/final_model/evaluate_result_with_rescore_option.py\n```\n\nThe above evaluation steps and code for\_`generate_result_files.py`\_are further documented in the colab notebook\_\n\n### Detailed evaluation results\n\nRefer to\_[Evaluation Results](https://github.com/AI4Bharat/IndicXlit#evaluation-results)\_for results of IndicXlit model on Dakshina and Aksharantar benchmarks. Please refer to section 7 of our\_[paper](https://arxiv.org/abs/2205.03018)\_for detailed discussion of the results\n\n## Finetuning the model on your input dataset\n\nThe high level steps for finetuning on your own dataset are:\n\nOrganize the train/test/valid data in corpus dir such that it has all the files containing parallel data for en-X lang pair in the following format\n\ntrain\\_x.en for training file of en-X lang pair which contains the space separated roman characters in each line\n\ntrain\\_x.x for training file of en-X lang pair which contains the space separated Indic characters in each line\n\n```\n# corpus/\r\n# ├── train_as.as\r\n# ├── train_en.en\r\n# ├── train_bn.bn\r\n# ├── train_en.en\r\n# ├── ....\r\n# ├── valid_as.as\r\n# ├── valid_en.en\r\n# ├── valid_bn.bn\r\n# ├── valid_en.en\r\n# ├── ....\r\n# ├── test_as.as\r\n# ├── test_en.en\r\n# ├── test_bn.bn\r\n# ├── test_en.en\r\n# └── ....\r\n\n```\n\nTo download and decompress the model file and joint vocabulary files use following commmand,\n\n```\n# download the IndicXlit models\r\nwget https://storage.googleapis.com/indic-xlit-public/final_model/indicxlit-en-indic-v1.0.zip\r\nunzip indicxlit-en-indic-v1.0.zip\n```\n\nbinarizing the files using the joint dictionaries\n\n```\nfor lang_abr in bn gu hi kn ml mr pa sd si ta te ur\r\ndo\r\n   fairseq-preprocess \\\r\n   --trainpref corpus/train_$lang_abr --validpref corpus/valid_$lang_abr --testpref corpus/test_$lang_abr \\\r\n   --srcdict corpus-bin/dict.en.txt \\\r\n   --tgtdict corpus-bin/dict.mlt.txt \\\r\n   --source-lang en --target-lang $lang_abr \\\r\n   --destdir corpus-bin \r\ndone\n```\n\nAdd all languages codes to\_`lang_list.txt`\_file and save it in the same dir\n\nPlease refer to fairseq documentaion to know more about each of these options ([https://fairseq.readthedocs.io/en/latest/command\\_line\\_tools.html](https://fairseq.readthedocs.io/en/latest/command\\_line\\_tools.html))\n\n```\n# We will use fairseq-train to finetune the model:\r\n# some notable args:\r\n# --lr                  -> learning rate. From our limited experiments, we find that lower learning rates like 3e-5 works best for finetuning.\r\n# --restore-file        -> reload the pretrained checkpoint and start training from here (change this path for indic-en. Currently its is set to en-indic)\r\n# --reset-*             -> reset and not use lr scheduler, dataloader, optimizer etc of the older checkpoint\r\n\r\nfairseq-train corpus-bin \\\r\n    --save-dir transformer \\\r\n    --arch transformer --layernorm-embedding \\\r\n    --task translation_multi_simple_epoch \\\r\n    --sampling-method \"temperature\" \\\r\n    --sampling-temperature 1.5 \\\r\n    --encoder-langtok \"tgt\" \\\r\n    --lang-dict lang_list.txt \\\r\n    --lang-pairs en-bn,en-gu,en-hi,en-kn,en-ml,en-mr,en-pa,en-sd,en-si,en-ta,en-te,en-ur \\\r\n    --decoder-normalize-before --encoder-normalize-before \\\r\n    --activation-fn gelu --adam-betas \"(0.9, 0.98)\"  \\\r\n    --batch-size 1024 \\\r\n    --decoder-attention-heads 4 --decoder-embed-dim 256 --decoder-ffn-embed-dim 1024 --decoder-layers 6 \\\r\n    --dropout 0.5 \\\r\n    --encoder-attention-heads 4 --encoder-embed-dim 256 --encoder-ffn-embed-dim 1024 --encoder-layers 6 \\\r\n    --lr 0.001 --lr-scheduler inverse_sqrt \\\r\n    --max-epoch 51 \\\r\n    --optimizer adam  \\\r\n    --num-workers 32 \\\r\n    --warmup-init-lr 0 --warmup-updates 4000 \\\r\n    --keep-last-epochs 5 \\\r\n    --patience 5 \\\r\n    --restore-file transformer/indicxlit.pt \\\r\n    --reset-lr-scheduler \\\r\n    --reset-meters \\\r\n    --reset-dataloader \\\r\n    --reset-optimizer\n```\n\nThe above steps (setup the environment, download the trained\_*IndicXlit*\_model and prepare your custom dataset for funetuning) are further documented in our colab notebook\_\n\n## Mining details\n\nFollowing links provides the detail description of mining from various resources,\n\n*   Samanantar:\_[https://github.com/AI4Bharat/IndicXlit/tree/master/data\\_mining/transliteration\\_mining\\_samanantar](https://github.com/AI4Bharat/IndicXlit/tree/master/data\\_mining/transliteration\\_mining\\_samanantar)\n*   IndicCorp:\_[https://github.com/AI4Bharat/IndicXlit/tree/master/data\\_mining/IndicCorp/skeleton/en\\_dict\\_workplace](https://github.com/AI4Bharat/IndicXlit/tree/master/data\\_mining/IndicCorp/skeleton/en\\_dict\\_workplace)\n\n## Directory structure\n\n```\nIndicXlit\r\n├── Checker\r\n│\_\_ ├── README.md\r\n│\_\_ ├── Transliteration_Checker.java\r\n│\_\_ └── Transliteration_Checker.py\r\n├── Dataset_Format\r\n│\_\_ ├── Create_Aksharantar_JSONL.py\r\n│\_\_ └── README.md\r\n├── LICENSE\r\n├── README.md\r\n├── ULCA_Format\r\n│\_\_ ├── README.md\r\n│\_\_ └── ULCA_dataset.py\r\n├── ablation_study\r\n│\_\_ ├── data_filteration\r\n│\_\_ │\_\_ ├── data_filteration_with_benchmark_test_dakshina_test_valid\r\n│\_\_ │\_\_ └── data_filteration_with_dakshina_test_valid\r\n│\_\_ └── model\r\n│\_\_     ├── monolingual_model\r\n│\_\_     ├── multilingual_model_(same for_singlescript_model)\r\n│\_\_     ├── north_model\r\n│\_\_     ├── preprocessing_for_rescoring\r\n│\_\_     ├── south_model\r\n│\_\_     └── specific_to_E_because_(differ_across_dataset_E_has_specific_langs)\r\n├── app\r\n│\_\_ ├── Caddyfile\r\n│\_\_ ├── Hosting.md\r\n│\_\_ ├── MANIFEST.in\r\n│\_\_ ├── README.md\r\n│\_\_ ├── ai4bharat\r\n│\_\_ │\_\_ ├── __init__.py\r\n│\_\_ │\_\_ └── transliteration\r\n│\_\_ ├── api_expose.py\r\n│\_\_ ├── auto_certif_renew.py\r\n│\_\_ ├── dependencies.txt\r\n│\_\_ ├── setup.py\r\n│\_\_ └── start_server.py\r\n├── corpus_preprocessing\r\n│\_\_ ├── Analysis\r\n│\_\_ │\_\_ ├── GIT_analysis.py\r\n│\_\_ │\_\_ ├── README.md\r\n│\_\_ │\_\_ └── len_stats.py\r\n│\_\_ ├── Benchmark_data_from_JSONS(Karya)\r\n│\_\_ │\_\_ ├── Benchmark_Named_entities.py\r\n│\_\_ │\_\_ ├── Benchmark_Transliteration_data.py\r\n│\_\_ │\_\_ └── README.md\r\n│\_\_ ├── Collating_existing_dataset\r\n│\_\_ │\_\_ ├── collate_data.ipynb\r\n│\_\_ │\_\_ ├── dataset_info.csv\r\n│\_\_ │\_\_ └── stats_detail.txt\r\n│\_\_ ├── Create_Unique_list_from_datasets\r\n│\_\_ │\_\_ ├── IndicCorp\r\n│\_\_ │\_\_ ├── LDCIL\r\n│\_\_ │\_\_ ├── README.md\r\n│\_\_ │\_\_ └── Words_freq_probability_after_kenlm\r\n│\_\_ └── Pre_process_arabic_scripts\r\n│\_\_     ├── README.md\r\n│\_\_     └── clean_urdu.py\r\n├── data_mining\r\n│\_\_ ├── IndicCorp\r\n│\_\_ │\_\_ ├── preprocess_data\r\n│\_\_ │\_\_ └── skeleton\r\n│\_\_ ├── readme.md\r\n│\_\_ └── transliteration_mining_samanantar\r\n│\_\_     ├── align_data.sh\r\n│\_\_     ├── convert_csv.py\r\n│\_\_     ├── extract_translit_pairs.sh\r\n│\_\_     ├── install_tools.txt\r\n│\_\_     ├── model_run_steps.txt\r\n│\_\_     ├── preprocess_data.py\r\n│\_\_     ├── readme.md\r\n│\_\_     ├── samanantar_pairs_count.xlsx\r\n│\_\_     └── validation_script.py\r\n├── inference\r\n│\_\_ ├── cli\r\n│\_\_ │\_\_ ├── generate_result_files.py\r\n│\_\_ │\_\_ ├── interactive.sh\r\n│\_\_ │\_\_ ├── lang_list.txt\r\n│\_\_ │\_\_ └── transliterate_word.sh\r\n│\_\_ └── python\r\n│\_\_     ├── custom_interactive.py\r\n│\_\_     ├── lang_list.txt\r\n│\_\_     ├── test_api_inference.py\r\n│\_\_     └── xlit_translit.py\r\n├── model_training_scripts\r\n│\_\_ ├── README.md\r\n│\_\_ ├── binarizing\r\n│\_\_ │\_\_ └── preprocess_all_lang.sh\r\n│\_\_ ├── data_filtration\r\n│\_\_ │\_\_ ├── combining_data_acrooss_lang.py\r\n│\_\_ │\_\_ ├── refresh_data_train_all_test_valid.py\r\n│\_\_ │\_\_ └── refresh_test_valid_data.py\r\n│\_\_ ├── evaluate\r\n│\_\_ │\_\_ ├── evaluate_result_with_rescore_option.py\r\n│\_\_ │\_\_ ├── final_result.sh\r\n│\_\_ │\_\_ └── final_result_without_rescoring.sh\r\n│\_\_ ├── generation\r\n│\_\_ │\_\_ ├── generate.sh\r\n│\_\_ │\_\_ └── generate_result_files.py\r\n│\_\_ ├── skeleton\r\n│\_\_ │\_\_ ├── blank_file.txt\r\n│\_\_ │\_\_ ├── creating_dir_struct.sh\r\n│\_\_ │\_\_ ├── indiccorp\r\n│\_\_ │\_\_ ├── mined_data\r\n│\_\_ │\_\_ ├── multi_lang\r\n│\_\_ │\_\_ ├── preprocess_data\r\n│\_\_ │\_\_ └── working\r\n│\_\_ ├── training\r\n│\_\_ │\_\_ ├── lang_list.txt\r\n│\_\_ │\_\_ └── train.sh\r\n│\_\_ └── vocab_creation\r\n│\_\_     └── preprocess.sh\r\n└── sample_images\r\n    ├── main_page.png\r\n    ├── select_language.png\r\n    └── transliterate_sentence.png\r\n\n```\n\n## Citing\n\nIf you are using any of the resources, please cite the following article:\n\n```\n@article{Madhani2022AksharantarTB,\r\n  title={Aksharantar: Towards building open transliteration tools for the next billion users},\r\n  author={Yash Madhani and Sushane Parthan and Priyanka A. Bedekar and Ruchi Khapra and Vivek Seshadri and Anoop Kunchukuttan and Pratyush Kumar and Mitesh M. Khapra},\r\n  journal={ArXiv},\r\n  year={2022},\r\n  volume={abs/2205.03018}\r\n}\r\n\n```\n\nWe would like to hear from you if:\n\n*   You are using our resources. Please let us know how you are putting these resources to use.\n*   You have any feedback on these resources.\n\n### License\n\nThe IndicXlit code (and models) are released under the MIT License.\n\n### Contributors\n\n*   Yash Madhani\_([AI4Bharat](https://ai4bharat.org/),\_[IITM](https://www.iitm.ac.in/))\n*   Sushane Parthan\_([AI4Bharat](https://ai4bharat.org/),\_[IITM](https://www.iitm.ac.in/))\n*   Priyanka Bedakar\_([AI4Bharat](https://ai4bharat.org/),\_[IITM](https://www.iitm.ac.in/))\n*   Ruchi Khapra\_([AI4Bharat](https://ai4bharat.org/))\n*   Gokul NC\_([AI4Bharat](https://ai4bharat.org/))\n*   Anoop Kunchukuttan\_([AI4Bharat](https://ai4bharat.org/),\_[Microsoft](https://www.microsoft.com/en-in/))\n*   Pratyush Kumar\_([AI4Bharat](https://ai4bharat.org/),\_[Microsoft](https://www.microsoft.com/en-in/),\_[IITM](https://www.iitm.ac.in/))\n*   Mitesh M. Khapra\_([AI4Bharat](https://ai4bharat.org/),\_[IITM](https://www.iitm.ac.in/))\n\n### Contact\n\n*   Anoop Kunchukuttan ([anoop.kunchukuttan@gmail.com](mailto:anoop.kunchukuttan@gmail.com))\n*   Mitesh Khapra ([miteshk@cse.iitm.ac.in](mailto:miteshk@cse.iitm.ac.in))\n*   Pratyush Kumar ([pratyush@cse.iitm.ac.in](mailto:pratyush@cse.iitm.ac.in))\n\n## Acknowledgements\n\nWe would like to thank EkStep Foundation for their generous grant which helped in setting up the Centre for AI4Bharat at IIT Madras to support our students, research staff, data and computational requirements. We would like to thank The Ministry of Electronics and Information Technology (NLTM) for its grant to support the creation of datasets and models for Indian languages under its ambitions Bhashini project. We would also like to thank the Centre for Development of Advanced Computing, India (C-DAC) for providing access to the Param Siddhi supercomputer for training our models. Lastly, we would like to thank Microsoft for its grant to create datasets, tools and resources for Indian languages.\n"
    _template: content
---

