---
blocks:
  - body: "[Speech Recognition](/speech-recognition) / [Models](/models) / [IndicWav2Vec](/indic-wav-2-vec)\n\n# IndicWav2Vec\n\nIndicWav2Vec is a multilingual speech model pretrained on 40 Indian langauges. This model represents the largest diversity of Indian languages in the pool of multilingual speech models. We fine-tune this model for downstream ASR for 9 languages and obtain state-of-the-art results on 3 public benchmarks, namely MUCS, MSR and OpenSLR.\n\nAs part of IndicWav2Vec we create largest publicly available corpora for 40 languages from 4 different language families. We also trained state-of-the-art ASR models for 9 Indian languages.\n\n## Benchmarks\n\nWe evaluate our models on 3 publicly available benchmarks MUCS, MSR and OpenSLR and below mentioned are our results\n\n**Modelgutateguhimrortatebnnesi**IndicW2V20.522.122.926.216.019.325.627.329.316.611.924.8IndicW2V + LM11.713.611.017.214.713.817.225.020.513.613.6-\n\n## Updates\n\n21 June 2022\n\n```\nAdded more documentation\r\n\n```\n\n## Table of contents\n\n*   [IndicWav2Vec](https://github.com/AI4Bharat/IndicWav2Vec#indicwav2vec)\n    *   [Benchmarks](https://github.com/AI4Bharat/IndicWav2Vec#benchmarks)\n    *   [Updates](https://github.com/AI4Bharat/IndicWav2Vec#updates)\n    *   [Table of contents](https://github.com/AI4Bharat/IndicWav2Vec#table-of-contents)\n    *   [Resources](https://github.com/AI4Bharat/IndicWav2Vec#resources)\n        *   [Download Models](https://github.com/AI4Bharat/IndicWav2Vec#download-models)\n        *   [Hosted API Usage](https://github.com/AI4Bharat/IndicWav2Vec#hosted-api-usage)\n        *   [Accessing on ULCA](https://github.com/AI4Bharat/IndicWav2Vec#accessing-on-ulca)\n    *   [Quick start](https://github.com/AI4Bharat/IndicWav2Vec#quick-start)\n        *   [Python Inference](https://github.com/AI4Bharat/IndicWav2Vec#python-inference)\n        *   [Huggingface Inference](https://github.com/AI4Bharat/IndicWav2Vec#huggingface-inference)\n    *   [Tutorials](https://github.com/AI4Bharat/IndicWav2Vec#tutorials)\n        *   [Setting up your environment](https://github.com/AI4Bharat/IndicWav2Vec#setting-up-your-environment)\n        *   [Pretraining](https://github.com/AI4Bharat/IndicWav2Vec#pretraining)\n            *   [Data preparation](https://github.com/AI4Bharat/IndicWav2Vec#data-preparation)\n            *   [Manifest Creation](https://github.com/AI4Bharat/IndicWav2Vec#manifest-creation)\n        *   [Training procedure and code](https://github.com/AI4Bharat/IndicWav2Vec#training-procedure-and-code)\n        *   [Finetuning](https://github.com/AI4Bharat/IndicWav2Vec#finetuning)\n            *   [Data preparation](https://github.com/AI4Bharat/IndicWav2Vec#data-preparation-1)\n            *   [Finetuning procedure and code](https://github.com/AI4Bharat/IndicWav2Vec#finetuning-procedure-and-code)\n            *   [Finetuning procedure and code](https://github.com/AI4Bharat/IndicWav2Vec#finetuning-procedure-and-code-1)\n        *   [Language Modelling (LM)](https://github.com/AI4Bharat/IndicWav2Vec#language-modelling-lm)\n            *   [Data preparation](https://github.com/AI4Bharat/IndicWav2Vec#data-preparation-2)\n            *   [Training details](https://github.com/AI4Bharat/IndicWav2Vec#training-details)\n        *   [Evaluating ASR models](https://github.com/AI4Bharat/IndicWav2Vec#evaluating-asr-models)\n        *   [Model exporting](https://github.com/AI4Bharat/IndicWav2Vec#model-exporting)\n        *   [Deployment](https://github.com/AI4Bharat/IndicWav2Vec#deployment)\n    *   [Cite](https://github.com/AI4Bharat/IndicWav2Vec#cite)\n    *   [License](https://github.com/AI4Bharat/IndicWav2Vec#license)\n    *   [Contributors](https://github.com/AI4Bharat/IndicWav2Vec#contributors)\n    *   [Contact](https://github.com/AI4Bharat/IndicWav2Vec#contact)\n\n## Resources\n\n### Download Models\n\nFinetuned Models\n\n**LanguageAcoustic ModelDictionaryLanguage ModelLexiconWandb**Bengali[fairseq](https://storage.googleapis.com/indicwav2vec-public/fine-tuning-ckpts/bengali\\_large.pt)\_|\_[\\[hf\\]](https://github.com/AI4Bharat/IndicWav2Vec/blob/main)[link](https://github.com/AI4Bharat/IndicWav2Vec/blob/main)[KenLM](https://storage.googleapis.com/indicwav2vec-public/language-models/bengali.zip)[link](https://storage.googleapis.com/indicwav2vec-public/language-models/bengali.zip)[link](https://github.com/AI4Bharat/IndicWav2Vec/blob/main)Gujarati[fairseq](https://storage.googleapis.com/indicwav2vec-public/fine-tuning-ckpts/gujarati\\_large.pt)\_/\_[hf](https://github.com/AI4Bharat/IndicWav2Vec/blob/main)[link](https://github.com/AI4Bharat/IndicWav2Vec/blob/main)[KenLM](https://storage.googleapis.com/indicwav2vec-public/language-models/guharati.zip)[link](https://storage.googleapis.com/indicwav2vec-public/language-models/guharati.zip)[link](https://github.com/AI4Bharat/IndicWav2Vec/blob/main)Hindi[fairseq](https://storage.googleapis.com/indicwav2vec-public/fine-tuning-ckpts/hindi\\_large.pt)\_/\_[hf](https://github.com/AI4Bharat/IndicWav2Vec/blob/main)[link](https://github.com/AI4Bharat/IndicWav2Vec/blob/main)[KenLM](https://storage.googleapis.com/indicwav2vec-public/language-models/hindi.zip)[link](https://storage.googleapis.com/indicwav2vec-public/language-models/hindi.zip)[link](https://github.com/AI4Bharat/IndicWav2Vec/blob/main)Marathi[fairseq](https://storage.googleapis.com/indicwav2vec-public/fine-tuning-ckpts/marathi\\_large.pt)\_/\_[hf](https://github.com/AI4Bharat/IndicWav2Vec/blob/main)[link](https://github.com/AI4Bharat/IndicWav2Vec/blob/main)[KenLM](https://storage.googleapis.com/indicwav2vec-public/language-models/marathi.zip)[link](https://storage.googleapis.com/indicwav2vec-public/language-models/marathi.zip)[link](https://github.com/AI4Bharat/IndicWav2Vec/blob/main)Nepali[fairseq](https://storage.googleapis.com/indicwav2vec-public/fine-tuning-ckpts/nepali\\_large.pt)\_/\_[hf](https://github.com/AI4Bharat/IndicWav2Vec/blob/main)[link](https://github.com/AI4Bharat/IndicWav2Vec/blob/main)[KenLM](https://storage.googleapis.com/indicwav2vec-public/language-models/nepali.zip)[link](https://storage.googleapis.com/indicwav2vec-public/language-models/nepali.zip)[link](https://github.com/AI4Bharat/IndicWav2Vec/blob/main)Odia[fairseq](https://storage.googleapis.com/indicwav2vec-public/fine-tuning-ckpts/bengali\\_large.pt)\_/\_[hf](https://github.com/AI4Bharat/IndicWav2Vec/blob/main)[link](https://github.com/AI4Bharat/IndicWav2Vec/blob/main)[KenLM](https://github.com/AI4Bharat/IndicWav2Vec/blob/main)[link](https://github.com/AI4Bharat/IndicWav2Vec/blob/main)[link](https://github.com/AI4Bharat/IndicWav2Vec/blob/main)Tamil[fairseq](https://storage.googleapis.com/indicwav2vec-public/fine-tuning-ckpts/odia\\_large.pt)\_/\_[hf](https://github.com/AI4Bharat/IndicWav2Vec/blob/main)[link](https://github.com/AI4Bharat/IndicWav2Vec/blob/main)[KenLM](https://storage.googleapis.com/indicwav2vec-public/language-models/odia.zip)[link](https://storage.googleapis.com/indicwav2vec-public/language-models/odia.zip)[link](https://github.com/AI4Bharat/IndicWav2Vec/blob/main)Telugu[fairseq](https://storage.googleapis.com/indicwav2vec-public/fine-tuning-ckpts/telugu\\_large.pt)\_/\_[hf](https://github.com/AI4Bharat/IndicWav2Vec/blob/main)[link](https://github.com/AI4Bharat/IndicWav2Vec/blob/main)[KenLM](https://storage.googleapis.com/indicwav2vec-public/language-models/telugu.zip)[link](https://storage.googleapis.com/indicwav2vec-public/language-models/telugu.zip)[link](https://github.com/AI4Bharat/IndicWav2Vec/blob/main)Sinhala[fairseq](https://storage.googleapis.com/indicwav2vec-public/fine-tuning-ckpts/sinhala\\_large.pt)\_/\_[hf](https://github.com/AI4Bharat/IndicWav2Vec/blob/main)[link](https://github.com/AI4Bharat/IndicWav2Vec/blob/main)[KenLM](https://github.com/AI4Bharat/IndicWav2Vec/blob/main)[link](https://github.com/AI4Bharat/IndicWav2Vec/blob/main)[link](https://github.com/AI4Bharat/IndicWav2Vec/blob/main)\n\nPretrained Model(\\*)\n\n**NameModel Checkpoint**IndicWav2Vec Large[fairseq](https://storage.googleapis.com/indicwav2vec-public/pretraining-ckpts/indicwav2vec-large.pt)IndicWav2Vec Base[fairseq](https://storage.googleapis.com/indicwav2vec-public/pretraining-ckpts/indicwav2vec-base.pt)\n\n(\\* Trained on 40 Indian Languages, more details can be found\_[here](https://www.aaai.org/AAAI22Papers/AAAI-12428.JavedT.pdf))\n\n### Hosted API Usage\n\nOur models are hosted at the following API end points.\n\n**LangugageLanguage CodeAPI End point**Bengalibn[https://34.65.180.101:5000/infer\\_ulca\\_bn](https://34.65.180.101:5000/infer\\_ulca\\_bn)Gujaratigu[https://34.65.180.101:5000/infer\\_ulca\\_gu](https://34.65.180.101:5000/infer\\_ulca\\_gu)Hindihi[https://216.48.182.174:4999/infer\\_ulca\\_hi](https://216.48.182.174:4999/infer\\_ulca\\_hi)Marathimr[https://34.65.180.101:5000/infer\\_ulca\\_mr](https://34.65.180.101:5000/infer\\_ulca\\_mr)Nepaline[https://34.65.180.101:5000/infer\\_ulca\\_ne](https://34.65.180.101:5000/infer\\_ulca\\_ne)Odiaor[https://34.65.180.101:5000/infer\\_ulca\\_or](https://34.65.180.101:5000/infer\\_ulca\\_or)Tamilta[https://34.65.180.101:5000/infer\\_ulca\\_ta](https://34.65.180.101:5000/infer\\_ulca\\_ta)Telugute[https://34.65.180.101:5000/infer\\_ulca\\_te](https://34.65.180.101:5000/infer\\_ulca\\_te)Sinhalasi[https://34.65.180.101:5000/infer\\_ulca\\_si](https://34.65.180.101:5000/infer\\_ulca\\_si)\n\nInput API data format\n\n```\n{\r\n    \"config\": {\r\n        \"language\":{\r\n          \"sourceLanguage\": \"#Language Code\"\r\n        },\r\n        \"transcriptionFormat\": {\"value\":\"transcript\"},\r\n        \"audioFormat\": \"wav\"\r\n    },\r\n    \"audio\": [{\r\n        \"audioContent\": \"#BASE64 Encoded String\"\r\n    }]\r\n}\r\n\r\nOR\r\n\r\n{\r\n    \"config\": {\r\n        \"language\":{\r\n          \"sourceLanguage\": \"#Language Code\"\r\n        },\r\n        \"transcriptionFormat\": {\"value\":\"transcript\"},\r\n        \"audioFormat\": \"wav\"\r\n    },\r\n    \"audio\": [{\r\n        \"audioUri\": \"#HTTP/GS path to file\"\r\n    }]\r\n}\r\n\r\n\n```\n\nOutput\n\n```\n{\r\n    \"output\": [\r\n        {\r\n            \"source\": \"सेकेंड स्टेप इस देसी है स्पेसिफाइड फॉरेस्ट राइट\"\r\n        }\r\n    ],\r\n    \"status\": \"SUCCESS\"\r\n}\r\n\n```\n\n### Accessing on ULCA\n\nOur models can be directly accessed on\_[ULCA](https://bhashini.gov.in/ulca/model/explore-models)\_by going into ASR section and filtering models by IndicWav2Vec.\n\n## Quick start\n\n### Python Inference\n\n*   python sfi.py \\[--audio-file AUDIO\\_FILE\\_PATH]           \\[--ft-model FT\\_MODEL]           \\[--w2l-decoder viterbi]\n*   python sfi.py \\[--audio-file AUDIO\\_FILE\\_PATH]             \\[--ft-model FT\\_MODEL\\_PATH]           \\[--w2l-decoder kenlm]           \\[--lexicon LEXICON\\_PATH]           \\[--kenlm-model KENLM\\_MODEL\\_PATH]          \\[--beam-threshold BEAM\\_THRESHOLD]           \\[--beam-size-token BEAM\\_SIZE\\_TOKEN]           \\[--beam BEAM\\_SIZE]           \\[--word-score WORD\\_SCORE]           \\[--lm-weight LM\\_WEIGHT]          \\[--unk-weight UNK\\_WEIGHT]           \\[--sil-weight SIL\\_WEIGHT]           \\[--nbest NBEST]\n\n### Huggingface Inference\n\n*   Coming soon\n\n## Tutorials\n\n### Setting up your environment\n\n*   conda create -n \\<env\\_name>conda activate \\<env\\_name>\n*   sudo apt-get install liblzma-dev libbz2-dev libzstd-dev libsndfile1-dev libopenblas-dev libfftw3-dev libgflags-dev libgoogle-glog-devsudo apt install build-essential cmake libboost-system-dev libboost-thread-dev libboost-program-options-dev libboost-test-dev libeigen3-dev zlib1g-dev libbz2-dev liblzma-dev ffmpeg pip install -r requirements.txt pip install packaging soundfile swifter editdistance omegaconf\n*   git clone https://github.com/AI4Bharat/fairseq.gitcd fairseqpip install --editable ./#\\[Optional for faster training]git clone https://github.com/NVIDIA/apexcd apexpip install -v --no-cache-dir --global-option=\"--cpp\\_ext\" --global-option=\"--cuda\\_ext\" \\\\--global-option=\"--deprecated\\_fused\\_adam\" --global-option=\"--xentropy\" \\\\--global-option=\"--fast\\_multihead\\_attn\" ./cd ..\n*   git clone https://github.com/kpu/kenlm.gitcd kenlmmkdir -p build && cd buildcmake .. make -j 16cd ..export KENLM\\_ROOT=$PWDcd ..\n*   git clone https://github.com/flashlight/flashlight.gitcd flashlight/bindings/pythonexport USE\\_MKL=0python setup.py install\n\n### Pretraining\n\n#### Data preparation\n\n*   bash dw\\_util.sh \\<path\\_to\\_urls> \\<data\\_store\\_path> \\<num\\_of\\_threads>The\_`<data_store_path>`\_refers to the location where the data will be downloaded. The\_`<num_of_threads>`\_can be used to control the parallelization.\n*   python vad.py \\<data\\_read\\_dir> \\<data\\_write\\_dir> \\<folder\\_name>The\_`<data_read_dir>`\_is the root of downloaded files which contain downloaded data in language-named-folders.The\_`<data_write_dir>`\_is the location for saving the data after VAD step.The\_`<folder_name>`\_refers to the names of language-named-folder for which you want to perform this VAD step.\\*The reason why folder\\_name has been kept as a seperate entity is to allow parallelization because one can process multiple folders simultaneously.\n*   python snr.py \\<data\\_path> \\<folder/language\\_name>where the\_`<data_path>`\_refers to the root path containing all the audios in language specific folders. Here it refers to the`\_<data_write_dir>`\_from the previous step. The\_`<folder/language_name>`\_refers to name of language\\_specific folder for which snr\\_filtering needs to be done. The audio data that is rejected is moved in the folder\_**\"snr\\_rejected\"**, which is created automatically.\n*   python chunking.py \\<chunking\\_path>All the audio files present in the\_`<chunking_path>`\_will be chunked and saved in the same location. The original files are\_**removed**.\n\nOr alternatively users can use the one single script\_`process_data.sh`\_to run the entire pipeline\n\n*   Usage:\_`bash process_data.sh </path/to/download> <num_of_threads>`\n*   The\_`</path/to/download>`\_refers to the location where the data will be downloaded.\n*   The\_`<num_of_threads>`\_can be used to control the parallelization.\n*   Please make sure that the relative path is urls directory is\_`../urls`\_from the script.\n\n#### Manifest Creation\n\nFor creating language-wise pretraining manifest\n\n```\npython path/to/lang_wise_manifest_creation.py /path/to/wave/files --dest /manifest/path --ext $ext --valid-percent $valid\r\n\n```\n\nFor\_`/path/to/wav/files/`\_we expect the directory to have one folder per language under the parent directory\n\nIn our pretraing, we use a\_`--valid-percent`\_as\_`0.03`\n\nFor creating a combined validation file for all languages, we concatenate all individual\_`*_valid.tsv`\_files to create a valid.tsv file.\n\n```\nimport pandas as pd\r\nimport glob\r\n\r\nfilenames = glob.glob(\"*_valid.tsv\")\r\n\r\ncombined = []\r\nfor f in filename:\r\n    df = pd.read_csv(f, skiprows=1, names=['f', 'd'], sep='\\t')\r\n    combined.append(df)\r\n\r\ndf_combined = pd.concat(combined, axis=0, ignore_index=True)\r\ndf_combined.to_csv('valid.tsv', index=True, header=False, sep='\\t')\r\n\n```\n\nWe then add the\_`/path/to/wav/files/`\_to the first line of the\_`valid.tsv`\_file\n\n### Training procedure and code\n\nFor pretraining the model we do multi-node training and schedule the runs with slurm.\n\nFollowing is the invocation script for training IndicWav2Vec base starting from Wav2Vec2.0 English base ckeckpoint\n\n```\nfairseq-hydra-train \\\r\n  task.data=/path/to/manifest/directory \\\r\n  common.wandb_project=<wandb project name> \\\r\n  task._name=temp_sampled_audio_pretraining \\\r\n  +task.sampling_alpha=0.7 \\\r\n  common.log_interval=200 \\\r\n  common.log_format=tqdm \\\r\n  dataset.max_tokens=3000000 \\\r\n  common.user_dir=/path/to/custom_task/directory \\\r\n  checkpoint.save_dir=/path/to/save/model/checkpoints \\\r\n  checkpoint.restore_file=/path/to wav2vec2-english-base/checkpoint.pt \\\r\n  +optimization.update_freq='[2]' \\\r\n  optimization.clip_norm=0.5 \\\r\n  checkpoint.reset_optimizer=true \\\r\n  distributed_training.distributed_world_size=<total GPUs> \\\r\n  distributed_training.distributed_port=$PORT \\\r\n  --config-dir /path/to/configs/directory \\\r\n  --config-name wav2vec2_base_librispeech\"\r\n\n```\n\nFor Large model we override the above configuration with\n\n```\n  checkpoint.restore_file=/path/to wav2vec2-english-large/checkpoint.pt \\\r\n  +optimization.update_freq='[6]' \\\r\n  lr_scheduler.warmup_updates=0 \\\r\n  --config-name wav2vec2_large_librivox\"\r\n\n```\n\nConfigs for both the models are provided in the configs directory\n\n### Finetuning\n\n#### Data preparation\n\n*   For datasets, that are not sampled uniformly at 16kHz, the user may run the following command to normalize the data first.bash normalize\\_sr.sh \\<path/to/the/folder/to/normalize> \\<ext|wav|mp3>\n*   Manifest creation\n    *   Make a new directory and name it (say\_`mucs`)\n    *   Note that the transcript.txt contain entries of the following type\\<filename1> \\<transcript1> #just the filename and not the path\\<filename2> \\<transcript2>\\<filename3> \\<transcript3>\\<filename4> \\<transcript4>...Sample structure of folder tree:mucs(or msr/openslr)    ├── hindi    │\_\_ ├── test    │\_\_ │\_\_ ├── audio    │\_\_ │\_\_ └── transcript.txt    │\_\_ ├── train    │\_\_ │\_\_ ├── audio    │\_\_ │\_\_ └── transcript.txt    │\_\_ └── valid    │\_\_     ├── audio    │\_\_     └── transcript.txt    └── marathi        ├── test        │\_\_ ├── audio        │\_\_ └── transcript.txt        ├── train        │\_\_ ├── audio        │\_\_ └── transcript.txt        └── valid            ├── audio            └── transcript.txt        .        .        .        .\n    *   bash m\\_process.sh \\<path/to/the/root/folder/(mucs)>The would result in creation of manifest folders in each language specific folder which can the be used with fairseq for finetuning.\n\n#### Finetuning procedure and code\n\nFollowing is the invocation script for finetuning IndicWav2Vec large on a particular language\n\n```\nfairseq-hydra-train \\\r\n  task.data=/path/to/finetune/manifest/directory/for/a/particular/language \\\r\n  common.wandb_project=<wandb project name> \\\r\n  model.w2v_path=/path/to/pretrained/model_large.pt \\\r\n  common.log_interval=50 \\\r\n  common.log_format=tqdm \\\r\n  dataset.max_tokens=1000000 \\\r\n  checkpoint.save_dir=/path/to/save/model/fine_tune_checkpoints \\\r\n  +optimization.update_freq='[1]' \\\r\n  distributed_training.distributed_world_size=<total GPUs> \\\r\n  --config-dir /path/to/configs/directory \\\r\n  --config-name ai4b_xlsr\"\r\n\n```\n\nFor IndicWav2Vec Base model we override the above configuration with\n\n```\n  model.w2v_path=/path/to/pretrained/model_base.pt \\\r\n  --config-name ai4b_base\"\r\n\n```\n\nConfigs for both the models are provided in the\_[finetune\\_configs](https://github.com/AI4Bharat/IndicWav2Vec/blob/main)\_directory\n\n#### Finetuning procedure and code\n\n### Language Modelling (LM)\n\nWe train 6-grams Statistical LM using\_[KenLM library](https://kheafield.com/code/kenlm/).\n\n#### Data preparation\n\n*   Prepare training manifest using\_[fairseq](https://github.com/pytorch/fairseq/tree/master/examples/wav2vec)\_and copy its path.\n*   Prepare clean\\_dump.txt containing\_`\"\\n\"`\_separated rows of text data.\n*   Add\_`dict.txt`\_containing\_`comma(,)`\_separated rows of characters and its' index.\n*   Add these two files to the\_`{lang}`\_folder, where\_`lang`\_denotes the language for which lm is to be trained.\n\n> Command to clean transcripts and prepare lexicon for training:\n\n```\npython utils/clean_corpus.py -d=<lm directory path> -l=<lang> --transcript=<speech transcript folder path> --st=<start code of lang> --en=<end code of lang> --top_k=<'k' most frequent words for vocab>\r\n\n```\n\n#### Training details\n\n> Run lm-training:\_`bash scripts/train_lm.sh <lm directory path> <lang>`.\n\nOuput will be generate at:\_`\"<lm directory path>/<lang>\"`.\n\n### Evaluating ASR models\n\n*   python3 fairseq/speech\\_recognition/infer.py $\\{manifest\\_path} --task audio\\_finetuning \\\\--nbest 1 --path $\\{checkpoint\\_path} --gen-subset $\\{valid|test} --results-path $\\{result\\_path} --w2l-decoder \\{viterbi | kenlm} \\\\--lm-weight 0 --word-score 0 --sil-weight 0 --criterion ctc --labels ltr --max-tokens 5000000 \\\\--post-process letterThis is default fairseq evaluation command and more documentation about this command can be seen\_[here](https://github.com/AI4Bharat/IndicWav2Vec/blob/main)\n*\n\n### Model exporting\n\n*   Huggingface\n*   ONNX/Torchscript\n\n### Deployment\n\n*   Server (Flask)\n    *   Install Flask\_`pip install flask flask-cors`\n    *   Change path for the acoustic models, decoding strategy, language models and lexicon in the Make path changes in\_`app/models_dict.json`\n    *   run server\_`python app/flask_dep.py`\n*   Server (Torchserve)\n    *   Coming soon\n*   Mobile\n    *   Coming soon\n\n## Cite\n\nPlease cite out work as:\n\n```\n@inproceedings{javed2021building,\r\n    title = {Towards Building ASR Systems for the Next Billion Users},\r\n    author = {Tahir Javed and Sumanth Doddapaneni and Abhigyan Raman and Kaushal Santosh Bhogale and Gowtham Ramesh and Anoop Kunchukuttan and Pratyush Kumar and Mitesh M. Khapra},\r\n    booktitle = \"Proceedings of the AAAI Conference on Artificial Intelligence\",\r\n    year = \"2022 (to appear)\",\r\n}\n```\n\n## License\n\nIndicWav2Vec is\_[MIT](https://choosealicense.com/licenses/mit/)-licensed. The license applies to all the pretrained, fine-tuned and language models\n\n## Contributors\n\n*   Tahir Javed, (IITM, AI4Bharat)\n*   Sumanth Doddapaneni, (AI4Bharat, RBCDSAI)\n*   Abhigyan Raman, (AI4Bharat)\n*   Kaushal Bhogale, (AI4Bharat)\n*   Gowtham Ramesh, (AI4Bharat, RBCDSAI)\n*   Anoop Kunchukuttan, (Microsoft, AI4Bharat)\n*   Pratyush Kumar, (Microsoft, AI4Bharat)\n*   Mitesh Khapra, (IITM, AI4Bharat, RBCDSAI)\n\n## Contact\n\n*   Anoop Kunchukuttan ([anoop.kunchukuttan@gmail.com](mailto:anoop.kunchukuttan@gmail.com))\n*   Mitesh Khapra ([miteshk@cse.iitm.ac.in](mailto:miteshk@cse.iitm.ac.in))\n*   Pratyush Kumar ([pratyush@cse.iitm.ac.in](mailto:pratyush@cse.iitm.ac.in))\n\n## Acknowledgements\n\nWe would like to thank EkStep Foundation for their generous grant which helped in setting up the Centre for AI4Bharat at IIT Madras to support our students, research staff, data and computational requirements. We would like to thank The Ministry of Electronics and Information Technology (NLTM) for its grant to support the creation of datasets and models for Indian languages under its ambitions Bhashini project. We would also like to thank the Centre for Development of Advanced Computing, India (C-DAC) for providing access to the Param Siddhi supercomputer for training our models. Lastly, we would like to thank Microsoft for its grant to create datasets, tools and resources for Indian languages.\n"
    _template: content
---

